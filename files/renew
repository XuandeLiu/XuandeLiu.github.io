# ---- 加载你的两个 DataFrame ----
import pandas as pd
training_df = pd.read_parquet('data/interim/training.parquet')
testing_df  = pd.read_parquet('data/interim/testing.parquet')

from data.uam_threeway_prep import ThreeWayConfig, make_threeway_loaders
dc = cfg['data']
twc = ThreeWayConfig(
    date_col=dc['date_col'], time_col=dc['time_col'], symbol_col=dc['symbol_col'], y_col=dc['y_col'],
    inner_split_date=dc['inner_split_date'], L_days=dc['L_days'], bars_per_day=dc['bars_per_day'],
    min_valid_ratio=dc['min_valid_ratio'], use_sparse=dc['use_sparse'],
    add_delta_g_feature=dc['add_delta_g_feature'], delta_cap=dc['delta_cap'],
    K_per_symbol=dc['K_per_symbol'], batch_size=dc['batch_size'],
    num_workers=dc['num_workers'], pin_memory=dc['pin_memory']
)

train_loader, inner_test_loader, outer_test_loader, meta = make_threeway_loaders(training_df, testing_df, twc)
in_features = meta['X'].shape[1]

# ---- 建模与优化器/调度/早停（启用我给你的训练循环即可）----
model = DeepLOBv(in_features=in_features, **cfg['model']).to(device)
opt = torch.optim.AdamW(model.parameters(), lr=cfg['train']['lr'], weight_decay=cfg['train']['weight_decay'])

sch_cfg = cfg['train']['scheduler']; scheduler = None
if sch_cfg['name'] == 'cosine':
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=sch_cfg['t_max'], eta_min=sch_cfg['min_lr'])
elif sch_cfg['name'] == 'plateau':
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='max',
        factor=sch_cfg['factor'], patience=sch_cfg['patience'], min_lr=sch_cfg['min_lr'])

scaler = GradScaler(enabled=cfg['train']['amp'])
logger = get_logger('train', cfg['paths']['logs_dir'])
best_metric, best_state = -1e9, None
patience, bad = cfg['train']['early_stopping']['patience'], 0
metric_name = cfg['train']['early_stopping']['metric']

from utils.metrics import masked_mse, masked_r2_last
from torch.cuda.amp import autocast
from tqdm import tqdm
import os, torch

def eval_loader(loader):
    model.eval()
    r2_sum = 0.0; n_batch = 0
    with torch.no_grad():
        for xb, yb, mb in loader:
            xb, yb, mb = xb.to(device), yb.to(device), mb.to(device)
            yh = model(xb)
            r2 = masked_r2_last(yh, yb, mb).item()
            r2_sum += r2; n_batch += 1
    return r2_sum / max(1, n_batch)

for ep in range(1, cfg['train']['epochs']+1):
    model.train()
    pbar = tqdm(train_loader, desc=f'Epoch {ep}/{cfg["train"]["epochs"]}')
    for xb, yb, mb in pbar:
        xb, yb, mb = xb.to(device), yb.to(device), mb.to(device)
        opt.zero_grad(set_to_none=True)
        with autocast(enabled=cfg['train']['amp']):
            yh = model(xb)
            loss = masked_mse(yh, yb, mb)
        scaler.scale(loss).backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg['train']['grad_clip'])
        scaler.step(opt); scaler.update()
        pbar.set_postfix(loss=float(loss.item()))

    # 用 inner_test 做早停监控
    val_metric = eval_loader(inner_test_loader)
    logger.info(f'Epoch {ep} | inner_test {metric_name}: {val_metric:.6f}')

    if scheduler is not None:
        if sch_cfg['name'] == 'plateau':
            scheduler.step(val_metric)
        else:
            scheduler.step()

    # 早停
    if val_metric > best_metric + cfg['train']['early_stopping']['min_delta']:
        best_metric, bad = val_metric, 0
        best_state = {k: v.cpu() for k, v in model.state_dict().items()}
        torch.save(best_state, os.path.join(cfg['paths']['checkpoints_dir'], 'best.pt'))
    else:
        bad += 1
        if cfg['train']['early_stopping']['enabled'] and bad >= patience:
            logger.info('Early stopping triggered.')
            break

# 保存最优指标
from utils.common import save_json
save_json({'best_inner_test_r2': best_metric}, f"{cfg['paths']['metrics_dir']}/train_summary.json")
logger.info(f'Best inner_test R2: {best_metric:.6f} (checkpoint saved).')




import pandas as pd, torch, os
from data.uam_threeway_prep import ThreeWayConfig, make_threeway_loaders
from models.deeplobv import DeepLOBv
from utils.metrics import masked_r2_last

dc = cfg['data']
twc = ThreeWayConfig(
    date_col=dc['date_col'], time_col=dc['time_col'], symbol_col=dc['symbol_col'], y_col=dc['y_col'],
    inner_split_date=dc['inner_split_date'], L_days=dc['L_days'], bars_per_day=dc['bars_per_day'],
    min_valid_ratio=dc['min_valid_ratio'], use_sparse=dc['use_sparse'],
    add_delta_g_feature=dc['add_delta_g_feature'], delta_cap=dc['delta_cap'],
    K_per_symbol=dc['K_per_symbol'], batch_size=dc['batch_size'],
    num_workers=dc['num_workers'], pin_memory=dc['pin_memory']
)

training_df = pd.read_parquet('data/interim/training.parquet')
testing_df  = pd.read_parquet('data/interim/testing.parquet')
train_loader, inner_test_loader, outer_test_loader, meta = make_threeway_loaders(training_df, testing_df, twc)

device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = DeepLOBv(in_features=meta['X'].shape[1], **cfg['model']).to(device)
state = torch.load(args.ckpt, map_location=device)
model.load_state_dict(state); model.eval()

@torch.no_grad()
def eval_loader(loader):
    r2_sum = 0.0; n_batch = 0
    for xb, yb, mb in loader:
        xb, yb, mb = xb.to(device), yb.to(device), mb.to(device)
        yh = model(xb)
        r2_sum += masked_r2_last(yh, yb, mb).item(); n_batch += 1
    return r2_sum / max(1, n_batch)

r2_inner = eval_loader(inner_test_loader)
r2_outer = eval_loader(outer_test_loader)
print(f'Inner-split R2: {r2_inner:.6f} | Outer-test (2023) R2: {r2_outer:.6f}')

from utils.common import save_json
save_json({'r2_inner': r2_inner, 'r2_outer': r2_outer}, os.path.join(cfg['paths']['metrics_dir'], 'eval_summary.json'))
